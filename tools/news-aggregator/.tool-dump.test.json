{"type":"Python","content":[{"name":"News Aggregator","tool_router_key":"local:::__official_shinkai:::news_aggregator","homepage":null,"author":"@@official.shinkai","version":"1.0.0","mcp_enabled":null,"py_code":"# /// script\n# requires-python = \"==3.10\"\n# dependencies = [\n#   \"requests==2.31.0\",\n#   \"newspaper3k==0.2.8\",\n#   \"aiohttp==3.9.1\",\n#   \"lxml==5.1.0\",\n#   \"beautifulsoup4==4.12.2\",\n#   \"nltk==3.8.1\",\n#   \"feedparser==6.0.10\",\n#   \"tldextract==5.1.1\",\n#   \"feedfinder2==0.0.4\",\n#   \"python-dateutil==2.8.2\",\n#   \"cssselect==1.2.0\"\n# ]\n# ///\n\nimport newspaper\nfrom newspaper import Article\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Dict, Any, Tuple\nimport asyncio\nimport aiohttp\nimport time\nimport feedparser\nfrom datetime import datetime\nimport logging\nimport traceback\nimport nltk\nimport os\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Download required NLTK data\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    logger.info(\"Downloading required NLTK data...\")\n    nltk.download('punkt', quiet=True)\n\n# RSS feed URLs for major news sites - using more reliable feeds\nRSS_FEEDS = {\n    \"reuters\": \"https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best\",\n    \"techcrunch\": \"https://techcrunch.com/feed\",\n    \"theverge\": \"https://www.theverge.com/rss/index.xml\",\n    \"wired\": \"https://www.wired.com/feed/rss\",\n    \"bloomberg\": \"https://www.bloomberg.com/feeds/sitemap_news.xml\"\n}\n\n# Default news providers by category if none specified\nDEFAULT_PROVIDERS = {\n    \"general\": [\n        \"https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best\",  # Use RSS feed URL directly\n        \"https://www.apnews.com\",\n        \"https://www.bbc.com\",\n    ],\n    \"tech\": [\n        \"https://techcrunch.com\",\n        \"https://www.theverge.com\",\n        \"https://www.wired.com\",\n    ],\n    \"business\": [\n        \"https://www.bloomberg.com\",\n        \"https://www.cnbc.com\",\n        \"https://www.forbes.com\",\n    ]\n}\n\n@dataclass\nclass AggregatedArticle:\n    title: str\n    url: str\n    source: str\n    summary: str\n    publish_date: str\n    authors: List[str]\n    top_image: str\n    text: str\n\nclass CONFIG:\n    \"\"\"Configuration for the multi-source news aggregator\"\"\"\n    language: str = \"en\"\n    number_threads: int = 10\n    request_timeout: int = 30\n    max_concurrent_sources: int = 5\n    \nclass INPUTS:\n    \"\"\"Input parameters for news aggregation\"\"\"\n    providers: List[str]  # List of news provider URLs\n    articles_per_source: Optional[int] = 5\n    categories: Optional[List[str]] = None  # If not provided, use all categories\n    \nclass OUTPUT:\n    \"\"\"Output structure containing aggregated news\"\"\"\n    total_sources_processed: int\n    total_articles_found: int\n    failed_sources: List[str]\n    articles: List[Dict[str, Any]]\n    processing_time: float\n\ndef format_date(date: datetime) -> str:\n    \"\"\"Format datetime object to ISO string or return empty string if None\"\"\"\n    if date:\n        return date.isoformat()\n    return \"\"\n\nasync def process_article(article: Article, source_url: str) -> Optional[AggregatedArticle]:\n    \"\"\"Process a single article with fallback for NLP failures\"\"\"\n    try:\n        article.download()\n        article.parse()\n        \n        # Try NLP but don't fail if it errors\n        try:\n            article.nlp()\n        except Exception as e:\n            logger.warning(f\"NLP processing failed for {article.url}: {str(e)}\")\n            # Create a basic summary from the first few sentences if NLP fails\n            text = article.text or \"\"\n            summary = \". \".join(text.split(\". \")[:3]) if text else \"\"\n        \n        return AggregatedArticle(\n            title=article.title or \"\",\n            url=article.url,\n            source=source_url,\n            summary=getattr(article, 'summary', \"\") or \"\",\n            publish_date=format_date(article.publish_date) if article.publish_date else \"\",\n            authors=article.authors or [],\n            top_image=article.top_image or \"\",\n            text=article.text or \"\"\n        )\n    except Exception as e:\n        logger.warning(f\"Failed to process article {article.url}: {str(e)}\")\n        return None\n\nasync def try_rss_feed(source_url: str, max_articles: int) -> List[AggregatedArticle]:\n    \"\"\"Try to fetch articles using RSS feed if available\"\"\"\n    try:\n        # Extract domain from URL\n        domain = source_url.split('www.')[-1].split('.com')[0] if 'www.' in source_url else source_url.split('.com')[0].split('//')[-1]\n        \n        if domain in RSS_FEEDS:\n            feed = feedparser.parse(RSS_FEEDS[domain])\n            articles = []\n            \n            for entry in feed.entries[:max_articles]:\n                try:\n                    # Create article object from feed entry\n                    article = Article(entry.link)\n                    processed = await process_article(article, source_url)\n                    if processed:\n                        articles.append(processed)\n                except Exception as e:\n                    logger.warning(f\"Error processing RSS entry {entry.link}: {str(e)}\")\n                    continue\n                    \n            return articles\n    except Exception as e:\n        logger.warning(f\"Error fetching RSS feed for {source_url}: {str(e)}\")\n    return []\n\nasync def process_source(source_url: str, max_articles: int, config: CONFIG) -> tuple[str, List[AggregatedArticle]]:\n    \"\"\"Process a single news source and return its articles\"\"\"\n    try:\n        # Configure newspaper for this source\n        newspaper.Config.language = config.language\n        newspaper.Config.number_threads = config.number_threads\n        newspaper.Config.request_timeout = 15  # Shorter timeout\n        newspaper.Config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n        newspaper.Config.fetch_images = False\n        newspaper.Config.memoize_articles = False\n        \n        # Additional headers for better access\n        newspaper.Config.headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'DNT': '1',\n            'Connection': 'keep-alive',\n            'Upgrade-Insecure-Requests': '1',\n            'Sec-Fetch-Dest': 'document',\n            'Sec-Fetch-Mode': 'navigate',\n            'Sec-Fetch-Site': 'none',\n            'Sec-Fetch-User': '?1',\n            'Cache-Control': 'max-age=0'\n        }\n        \n        # Special handling for Reuters - always use RSS feed\n        if \"reuters.com\" in source_url:\n            source_url = RSS_FEEDS[\"reuters\"]\n            \n        # Ensure URL uses HTTPS\n        if not source_url.startswith('https://'):\n            source_url = source_url.replace('http://', 'https://')\n            if not source_url.startswith('https://'):\n                source_url = 'https://' + source_url\n\n        # Add www if not present and not an RSS feed\n        if not source_url.startswith('https://www.') and 'feed' not in source_url:\n            source_url = source_url.replace('https://', 'https://www.')\n        \n        # First try RSS feed\n        articles = await try_rss_feed(source_url, max_articles)\n        if articles:\n            logger.info(f\"Successfully fetched {len(articles)} articles via RSS from {source_url}\")\n            return source_url, articles\n            \n        # If RSS fails, try direct scraping\n        logger.info(f\"Attempting direct scraping for {source_url}\")\n        \n        # Build paper\n        paper = newspaper.build(source_url, \n                              language=config.language,\n                              memoize_articles=False)\n        \n        articles = []\n        for i, article in enumerate(paper.articles):\n            if i >= max_articles:\n                break\n                \n            processed = await process_article(article, source_url)\n            if processed:\n                articles.append(processed)\n                logger.info(f\"Successfully processed article: {article.url}\")\n                \n        if articles:\n            logger.info(f\"Successfully fetched {len(articles)} articles via scraping from {source_url}\")\n        else:\n            logger.warning(f\"No articles found for {source_url}\")\n            \n        return source_url, articles\n    except Exception as e:\n        logger.error(f\"Failed to process {source_url}: {str(e)}\\n{traceback.format_exc()}\")\n        return source_url, []\n\nasync def run(config: CONFIG, inputs: INPUTS) -> OUTPUT:\n    \"\"\"\n    Aggregate latest news from multiple sources in parallel.\n    \"\"\"\n    start_time = time.time()\n    output = OUTPUT()\n    output.failed_sources = []\n    output.articles = []\n    \n    # Use default providers if none specified\n    providers = inputs.providers\n    if not providers:\n        if inputs.categories:\n            providers = []\n            for category in inputs.categories:\n                if category.lower() in DEFAULT_PROVIDERS:\n                    providers.extend(DEFAULT_PROVIDERS[category.lower()])\n        else:\n            # Use all default providers if no categories specified\n            providers = [url for urls in DEFAULT_PROVIDERS.values() for url in urls]\n    \n    # Process sources in parallel with rate limiting\n    semaphore = asyncio.Semaphore(config.max_concurrent_sources)\n    async with semaphore:\n        tasks = []\n        for source_url in providers:\n            task = asyncio.create_task(\n                process_source(\n                    source_url,\n                    inputs.articles_per_source,\n                    config\n                )\n            )\n            tasks.append(task)\n        \n        # Wait for all tasks to complete\n        results = await asyncio.gather(*tasks)\n    \n    # Process results\n    successful_sources = 0\n    total_articles = 0\n    \n    for source_url, articles in results:\n        if articles:\n            successful_sources += 1\n            total_articles += len(articles)\n            \n            # Convert articles to dictionary format\n            for article in articles:\n                output.articles.append({\n                    \"title\": article.title,\n                    \"url\": article.url,\n                    \"source\": article.source,\n                    \"summary\": article.summary,\n                    \"publish_date\": article.publish_date,\n                    \"authors\": article.authors,\n                    \"top_image\": article.top_image,\n                    \"text\": article.text\n                })\n        else:\n            output.failed_sources.append(source_url)\n    \n    # Sort articles by publish date (most recent first)\n    output.articles.sort(\n        key=lambda x: x[\"publish_date\"] if x[\"publish_date\"] else \"0\",\n        reverse=True\n    )\n    \n    output.total_sources_processed = successful_sources\n    output.total_articles_found = total_articles\n    output.processing_time = time.time() - start_time\n    \n    return output ","tools":[],"config":[{"BasicConfig":{"key_name":"language","description":"The language to use for article processing","required":false,"type":null,"key_value":null}},{"BasicConfig":{"key_name":"number_threads","description":"Number of threads for article processing","required":false,"type":null,"key_value":null}},{"BasicConfig":{"key_name":"request_timeout","description":"Timeout in seconds for HTTP requests","required":false,"type":null,"key_value":null}},{"BasicConfig":{"key_name":"max_concurrent_sources","description":"Maximum number of sources to process concurrently","required":false,"type":null,"key_value":null}}],"description":"Aggregates latest news from multiple sources using newspaper3k, supporting parallel processing and category-based filtering","keywords":["news","aggregator","newspaper3k","scraper","articles","multi-source"],"input_args":{"type":"object","properties":{"providers":{"type":"array","description":"List of news provider URLs to aggregate from","items":{"type":"string","description":"URL of the news provider"}},"articles_per_source":{"type":"integer","description":"Maximum number of articles to fetch per source"},"categories":{"type":"array","description":"Categories to fetch news from (uses default providers if no specific providers given)","items":{"type":"string","description":"News category"}}},"required":["providers"]},"output_arg":{"json":""},"activated":false,"embedding":[0.47527418,0.49519074,-0.7538863,-0.4212616,-0.18234454,-0.34448063,0.059085328,-0.32761076,-0.5534749,-0.07620872,-0.25238493,0.61145025,0.62125564,-0.21722358,-0.19603352,-0.49246088,-0.06198502,-0.58336014,-1.9529483,-0.21810669,0.9386211,0.28264034,0.18883002,0.18067636,0.7116923,-0.0076182135,-0.28765294,-0.61059624,-0.46519256,-2.1860833,0.6002676,0.5524213,-0.576489,-0.19711944,-0.313143,-0.60847616,-0.38721514,-0.43877625,-0.41035277,-0.13567968,0.57566774,0.31550622,-0.24599224,0.2680277,0.23774698,-0.26878467,0.17426097,-0.26987505,0.30446282,0.4284321,-0.06757904,-0.35414356,-0.45872656,-0.13967295,-0.35705078,-0.21734005,0.21274988,0.47039837,-0.09445876,0.16206029,0.050005022,0.21344623,-4.0009904,0.07280558,0.042618107,-0.03965096,0.31232622,0.16896762,-0.29443112,0.46851724,0.31557506,0.43290204,0.2726605,0.22491637,-0.29349136,-0.74448663,0.47236532,-0.57837284,-0.08021554,-0.4313789,0.51012415,0.5452733,-0.45932165,-0.30023086,-0.44481328,0.6635115,-0.8652026,-0.5519564,0.20704186,-0.48249036,0.20879604,0.052368924,0.071731746,-0.19401565,-0.15196717,0.553267,-0.15939035,0.71715814,0.24218091,3.3998632,0.5470173,-0.058587372,0.1911577,-0.52095884,0.45489442,-0.776384,0.106699705,-0.08337189,0.10413424,0.081272565,-0.032244988,-0.21966958,0.46645567,-0.09468998,0.22041154,0.6022555,-0.47325838,0.11427498,0.014999554,0.37607488,0.031354845,0.040197425,-0.3008697,-0.34170428,-0.06419781,0.20934689,-0.2656243,0.28855535,0.24815588,-0.32630226,0.21609107,-0.8918428,-1.2010144,-0.37839878,-0.17320198,0.32339087,0.81068194,-0.25360137,0.22744012,-1.1780633,0.004944995,-1.149202,0.5928466,-0.07877384,0.7214194,0.6053432,-0.25391042,0.15271652,-0.84942687,0.4932752,-0.1594581,0.46431825,0.1489443,-0.17156895,1.0943149,-0.18670462,-0.014962632,0.28059053,-0.73315585,0.08046925,-0.10481647,0.26519546,0.53085715,-0.10225341,0.0077042207,-0.70785344,0.4113567,-0.22999752,0.6222755,0.17011356,0.43995398,0.044257663,0.112146236,0.82947445,-0.26231843,-0.08841654,-0.4353077,-0.29158232,0.6994337,-0.25657994,0.44458675,0.9730542,-0.38059565,-0.20769373,-0.48994178,0.65264416,0.152641,0.28987217,1.0759288,0.98529744,-0.18583357,1.4821492,-0.56112635,-0.18551578,0.040210515,-0.06153172,-0.5403865,0.16861343,0.91607296,0.44447613,0.15512401,0.2610703,-0.23992582,-0.10404749,0.23500219,-0.70620567,0.4065421,-0.38287082,0.5346595,-0.76639175,0.01917198,0.16378634,0.34111995,0.14296952,0.10911867,-0.07706131,0.053512357,-0.14791542,0.4967789,1.0014533,0.3309345,0.20490476,-0.074934185,-0.7055149,-0.5916813,0.023272205,-0.78413016,-0.20879205,-0.082618006,0.3828762,0.73030776,0.4267353,0.23859733,0.49079388,0.6517684,-0.1177466,0.31863993,0.8237865,0.4104561,-0.3138679,0.36065945,0.22004153,-0.45471072,0.21425952,0.468988,-0.010357022,-0.034205854,-0.11040085,0.19211727,1.6251848,0.43424612,-0.14749433,0.51632655,0.37043932,0.20210531,-0.2923644,-1.0827686,0.09926749,-0.2926352,0.3885878,-0.55711204,-0.6148907,0.40864572,0.47505853,0.085201226,-0.04023486,-0.07377359,0.07537063,0.19718763,-0.2484591,0.10351162,0.112515315,-0.10275872,-0.22226265,0.018283796,0.24471793,0.58753604,-0.0013665408,-0.67146486,-0.49834755,0.84076744,-0.07686066,-0.26682156,0.28032184,-0.6765478,-0.26362646,-0.66866666,0.18982819,-0.54890025,0.86511934,-0.15887348,-0.8386669,-0.10998717,-0.53987205,1.6947505,0.038967155,0.49053684,0.54211396,-0.03991039,-0.2973076,-0.15154015,0.4327915,-0.29332542,-0.26325652,-0.2683205,-0.6937869,0.5412531,-0.21920437,-0.12129559,0.25470927,-0.38262558,-0.14858562,0.41394082,-0.15174583,0.7789284,-0.52114075,-0.052612122,0.5293473,0.2396455,-2.56369,-0.36457452,0.08866228,0.16940017,-0.20451565,0.014381202,0.9041508,-0.2419382,0.04362876,-0.5923305,0.89467204,0.22009198,-0.08071969,-0.7160903,0.1268323,0.5445866,-0.4192806,-0.3704002,-0.3009395,-0.6221858,-0.36152327,-0.19381285,1.4530237,0.31915152,0.17356522,-0.37237754,0.5885845,-0.5304917,-0.90742624,1.0581019,0.3770724,-0.11098363,0.43091682,-0.47646645,-0.6594271,0.1734962,1.352688,0.07970886,-0.07832716,-0.24393585,1.229089,-0.5183264,-0.1309066,-0.007925101,0.60560226,0.055418447,0.34786195,0.32026622,-0.018077478,0.09807718,0.0037403032,-0.28533614,-0.4237845,-0.0010849908,-0.2991575,0.5400162,-0.17255764,0.6232747,0.61270547,0.18797271,0.022751026,-0.32999182,-0.13466461,-0.8565352,0.3611194],"result":{"type":"object","properties":{"articles":{"items":{"properties":{"authors":{"items":{"type":"string"},"type":"array"},"publish_date":{"type":"string"},"source":{"type":"string"},"summary":{"type":"string"},"text":{"type":"string"},"title":{"type":"string"},"top_image":{"type":"string"},"url":{"type":"string"}},"type":"object"},"type":"array"},"failed_sources":{"items":{"type":"string"},"type":"array"},"processing_time":{"type":"number"},"total_articles_found":{"description":"Total number of articles found across all sources","type":"integer"},"total_sources_processed":{"description":"Number of news sources that were processed","type":"integer"}},"required":["total_sources_processed","total_articles_found","failed_sources","articles","processing_time"]},"sql_tables":null,"sql_queries":null,"file_inbox":null,"oauth":null,"assets":null,"runner":"any","operating_system":["linux","macos","windows"],"tool_set":""},false]}