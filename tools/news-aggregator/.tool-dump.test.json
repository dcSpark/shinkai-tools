{"type":"Python","content":[{"version":"1.0.0","name":"News Aggregator","homepage":null,"author":"@@official.shinkai","py_code":"# /// script\n# requires-python = \"==3.10\"\n# dependencies = [\n#   \"requests==2.31.0\",\n#   \"newspaper3k==0.2.8\",\n#   \"aiohttp==3.9.1\",\n#   \"lxml==5.1.0\",\n#   \"beautifulsoup4==4.12.2\",\n#   \"nltk==3.8.1\",\n#   \"feedparser==6.0.10\",\n#   \"tldextract==5.1.1\",\n#   \"feedfinder2==0.0.4\",\n#   \"python-dateutil==2.8.2\",\n#   \"cssselect==1.2.0\"\n# ]\n# ///\n\nimport newspaper\nfrom newspaper import Article\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Dict, Any, Tuple\nimport asyncio\nimport aiohttp\nimport time\nimport feedparser\nfrom datetime import datetime\nimport logging\nimport traceback\nimport nltk\nimport os\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Download required NLTK data\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    logger.info(\"Downloading required NLTK data...\")\n    nltk.download('punkt', quiet=True)\n\n# RSS feed URLs for major news sites - using more reliable feeds\nRSS_FEEDS = {\n    \"reuters\": \"https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best\",\n    \"techcrunch\": \"https://techcrunch.com/feed\",\n    \"theverge\": \"https://www.theverge.com/rss/index.xml\",\n    \"wired\": \"https://www.wired.com/feed/rss\",\n    \"bloomberg\": \"https://www.bloomberg.com/feeds/sitemap_news.xml\"\n}\n\n# Default news providers by category if none specified\nDEFAULT_PROVIDERS = {\n    \"general\": [\n        \"https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best\",  # Use RSS feed URL directly\n        \"https://www.apnews.com\",\n        \"https://www.bbc.com\",\n    ],\n    \"tech\": [\n        \"https://techcrunch.com\",\n        \"https://www.theverge.com\",\n        \"https://www.wired.com\",\n    ],\n    \"business\": [\n        \"https://www.bloomberg.com\",\n        \"https://www.cnbc.com\",\n        \"https://www.forbes.com\",\n    ]\n}\n\n@dataclass\nclass AggregatedArticle:\n    title: str\n    url: str\n    source: str\n    summary: str\n    publish_date: str\n    authors: List[str]\n    top_image: str\n    text: str\n\nclass CONFIG:\n    \"\"\"Configuration for the multi-source news aggregator\"\"\"\n    language: str = \"en\"\n    number_threads: int = 10\n    request_timeout: int = 30\n    max_concurrent_sources: int = 5\n    \nclass INPUTS:\n    \"\"\"Input parameters for news aggregation\"\"\"\n    providers: List[str]  # List of news provider URLs\n    articles_per_source: Optional[int] = 5\n    categories: Optional[List[str]] = None  # If not provided, use all categories\n    \nclass OUTPUT:\n    \"\"\"Output structure containing aggregated news\"\"\"\n    total_sources_processed: int\n    total_articles_found: int\n    failed_sources: List[str]\n    articles: List[Dict[str, Any]]\n    processing_time: float\n\ndef format_date(date: datetime) -> str:\n    \"\"\"Format datetime object to ISO string or return empty string if None\"\"\"\n    if date:\n        return date.isoformat()\n    return \"\"\n\nasync def process_article(article: Article, source_url: str) -> Optional[AggregatedArticle]:\n    \"\"\"Process a single article with fallback for NLP failures\"\"\"\n    try:\n        article.download()\n        article.parse()\n        \n        # Try NLP but don't fail if it errors\n        try:\n            article.nlp()\n        except Exception as e:\n            logger.warning(f\"NLP processing failed for {article.url}: {str(e)}\")\n            # Create a basic summary from the first few sentences if NLP fails\n            text = article.text or \"\"\n            summary = \". \".join(text.split(\". \")[:3]) if text else \"\"\n        \n        return AggregatedArticle(\n            title=article.title or \"\",\n            url=article.url,\n            source=source_url,\n            summary=getattr(article, 'summary', \"\") or \"\",\n            publish_date=format_date(article.publish_date) if article.publish_date else \"\",\n            authors=article.authors or [],\n            top_image=article.top_image or \"\",\n            text=article.text or \"\"\n        )\n    except Exception as e:\n        logger.warning(f\"Failed to process article {article.url}: {str(e)}\")\n        return None\n\nasync def try_rss_feed(source_url: str, max_articles: int) -> List[AggregatedArticle]:\n    \"\"\"Try to fetch articles using RSS feed if available\"\"\"\n    try:\n        # Extract domain from URL\n        domain = source_url.split('www.')[-1].split('.com')[0] if 'www.' in source_url else source_url.split('.com')[0].split('//')[-1]\n        \n        if domain in RSS_FEEDS:\n            feed = feedparser.parse(RSS_FEEDS[domain])\n            articles = []\n            \n            for entry in feed.entries[:max_articles]:\n                try:\n                    # Create article object from feed entry\n                    article = Article(entry.link)\n                    processed = await process_article(article, source_url)\n                    if processed:\n                        articles.append(processed)\n                except Exception as e:\n                    logger.warning(f\"Error processing RSS entry {entry.link}: {str(e)}\")\n                    continue\n                    \n            return articles\n    except Exception as e:\n        logger.warning(f\"Error fetching RSS feed for {source_url}: {str(e)}\")\n    return []\n\nasync def process_source(source_url: str, max_articles: int, config: CONFIG) -> tuple[str, List[AggregatedArticle]]:\n    \"\"\"Process a single news source and return its articles\"\"\"\n    try:\n        # Configure newspaper for this source\n        newspaper.Config.language = config.language\n        newspaper.Config.number_threads = config.number_threads\n        newspaper.Config.request_timeout = 15  # Shorter timeout\n        newspaper.Config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n        newspaper.Config.fetch_images = False\n        newspaper.Config.memoize_articles = False\n        \n        # Additional headers for better access\n        newspaper.Config.headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'DNT': '1',\n            'Connection': 'keep-alive',\n            'Upgrade-Insecure-Requests': '1',\n            'Sec-Fetch-Dest': 'document',\n            'Sec-Fetch-Mode': 'navigate',\n            'Sec-Fetch-Site': 'none',\n            'Sec-Fetch-User': '?1',\n            'Cache-Control': 'max-age=0'\n        }\n        \n        # Special handling for Reuters - always use RSS feed\n        if \"reuters.com\" in source_url:\n            source_url = RSS_FEEDS[\"reuters\"]\n            \n        # Ensure URL uses HTTPS\n        if not source_url.startswith('https://'):\n            source_url = source_url.replace('http://', 'https://')\n            if not source_url.startswith('https://'):\n                source_url = 'https://' + source_url\n\n        # Add www if not present and not an RSS feed\n        if not source_url.startswith('https://www.') and 'feed' not in source_url:\n            source_url = source_url.replace('https://', 'https://www.')\n        \n        # First try RSS feed\n        articles = await try_rss_feed(source_url, max_articles)\n        if articles:\n            logger.info(f\"Successfully fetched {len(articles)} articles via RSS from {source_url}\")\n            return source_url, articles\n            \n        # If RSS fails, try direct scraping\n        logger.info(f\"Attempting direct scraping for {source_url}\")\n        \n        # Build paper\n        paper = newspaper.build(source_url, \n                              language=config.language,\n                              memoize_articles=False)\n        \n        articles = []\n        for i, article in enumerate(paper.articles):\n            if i >= max_articles:\n                break\n                \n            processed = await process_article(article, source_url)\n            if processed:\n                articles.append(processed)\n                logger.info(f\"Successfully processed article: {article.url}\")\n                \n        if articles:\n            logger.info(f\"Successfully fetched {len(articles)} articles via scraping from {source_url}\")\n        else:\n            logger.warning(f\"No articles found for {source_url}\")\n            \n        return source_url, articles\n    except Exception as e:\n        logger.error(f\"Failed to process {source_url}: {str(e)}\\n{traceback.format_exc()}\")\n        return source_url, []\n\nasync def run(config: CONFIG, inputs: INPUTS) -> OUTPUT:\n    \"\"\"\n    Aggregate latest news from multiple sources in parallel.\n    \"\"\"\n    start_time = time.time()\n    output = OUTPUT()\n    output.failed_sources = []\n    output.articles = []\n    \n    # Use default providers if none specified\n    providers = inputs.providers\n    if not providers:\n        if inputs.categories:\n            providers = []\n            for category in inputs.categories:\n                if category.lower() in DEFAULT_PROVIDERS:\n                    providers.extend(DEFAULT_PROVIDERS[category.lower()])\n        else:\n            # Use all default providers if no categories specified\n            providers = [url for urls in DEFAULT_PROVIDERS.values() for url in urls]\n    \n    # Process sources in parallel with rate limiting\n    semaphore = asyncio.Semaphore(config.max_concurrent_sources)\n    async with semaphore:\n        tasks = []\n        for source_url in providers:\n            task = asyncio.create_task(\n                process_source(\n                    source_url,\n                    inputs.articles_per_source,\n                    config\n                )\n            )\n            tasks.append(task)\n        \n        # Wait for all tasks to complete\n        results = await asyncio.gather(*tasks)\n    \n    # Process results\n    successful_sources = 0\n    total_articles = 0\n    \n    for source_url, articles in results:\n        if articles:\n            successful_sources += 1\n            total_articles += len(articles)\n            \n            # Convert articles to dictionary format\n            for article in articles:\n                output.articles.append({\n                    \"title\": article.title,\n                    \"url\": article.url,\n                    \"source\": article.source,\n                    \"summary\": article.summary,\n                    \"publish_date\": article.publish_date,\n                    \"authors\": article.authors,\n                    \"top_image\": article.top_image,\n                    \"text\": article.text\n                })\n        else:\n            output.failed_sources.append(source_url)\n    \n    # Sort articles by publish date (most recent first)\n    output.articles.sort(\n        key=lambda x: x[\"publish_date\"] if x[\"publish_date\"] else \"0\",\n        reverse=True\n    )\n    \n    output.total_sources_processed = successful_sources\n    output.total_articles_found = total_articles\n    output.processing_time = time.time() - start_time\n    \n    return output ","tools":[],"config":[{"BasicConfig":{"key_name":"language","description":"The language to use for article processing","required":false,"type":null,"key_value":null}},{"BasicConfig":{"key_name":"number_threads","description":"Number of threads for article processing","required":false,"type":null,"key_value":null}},{"BasicConfig":{"key_name":"request_timeout","description":"Timeout in seconds for HTTP requests","required":false,"type":null,"key_value":null}},{"BasicConfig":{"key_name":"max_concurrent_sources","description":"Maximum number of sources to process concurrently","required":false,"type":null,"key_value":null}}],"description":"Aggregates latest news from multiple sources using newspaper3k, supporting parallel processing and category-based filtering","keywords":["news","aggregator","newspaper3k","scraper","articles","multi-source"],"input_args":{"type":"object","properties":{"categories":{"type":"array","description":"Categories to fetch news from (uses default providers if no specific providers given)","items":{"type":"string","description":"News category"}},"providers":{"type":"array","description":"List of news provider URLs to aggregate from","items":{"type":"string","description":"URL of the news provider"}},"articles_per_source":{"type":"integer","description":"Maximum number of articles to fetch per source"}},"required":["providers"]},"output_arg":{"json":""},"activated":false,"embedding":[0.47526938,0.4953609,-0.7539297,-0.4213634,-0.18249615,-0.3445613,0.059170797,-0.32747915,-0.55344987,-0.07627457,-0.25242993,0.61151814,0.6212426,-0.2172963,-0.19604032,-0.49254626,-0.06184599,-0.5833528,-1.952888,-0.21834213,0.9383628,0.28269887,0.18889183,0.18071964,0.71191776,-0.0076759034,-0.28750116,-0.61064374,-0.465151,-2.1860697,0.600066,0.5523878,-0.57666177,-0.19712755,-0.31296527,-0.60839844,-0.38727087,-0.43875867,-0.4103421,-0.13580273,0.57579815,0.31535262,-0.24596766,0.26801836,0.23782909,-0.2687323,0.17434858,-0.2696924,0.304609,0.42853475,-0.06755791,-0.35422504,-0.458871,-0.1397486,-0.35692644,-0.21732676,0.21275057,0.47052473,-0.09455772,0.16207723,0.050200723,0.21347103,-4.0008683,0.07265331,0.042692687,-0.03948301,0.31243417,0.16889101,-0.29429957,0.468549,0.31565982,0.43296093,0.27258793,0.22502348,-0.29342496,-0.7445577,0.47258988,-0.5784571,-0.08010504,-0.43132627,0.51002,0.5450627,-0.45935446,-0.30024526,-0.44491336,0.6636507,-0.86530024,-0.55189574,0.20723236,-0.48232386,0.20877907,0.052227266,0.07153457,-0.19411328,-0.15183812,0.5532529,-0.15942256,0.7170253,0.24215907,3.399865,0.5470605,-0.058721747,0.19130938,-0.5211649,0.45476967,-0.7763161,0.10674085,-0.083470814,0.10409772,0.08124617,-0.032256164,-0.21957931,0.46640643,-0.09472006,0.22037272,0.60221195,-0.47335482,0.1141192,0.015111724,0.37610847,0.031500325,0.04028113,-0.30098668,-0.34179255,-0.06408883,0.20924407,-0.2656547,0.2886469,0.24822132,-0.32637447,0.2162486,-0.8918812,-1.2007765,-0.37820992,-0.17327838,0.3234111,0.8108251,-0.2535401,0.22747561,-1.1781744,0.0049649775,-1.14906,0.59274274,-0.078641266,0.72137225,0.6054177,-0.25390625,0.15279242,-0.84940904,0.49320555,-0.15944539,0.46448833,0.14899099,-0.17157373,1.0941906,-0.18673392,-0.014947396,0.2806765,-0.7332697,0.08063069,-0.104901776,0.2650577,0.53099155,-0.10254942,0.00782774,-0.7080512,0.41145474,-0.22980839,0.62229615,0.1702511,0.44004092,0.044538505,0.11197571,0.8295875,-0.26212522,-0.08837201,-0.43531528,-0.29162502,0.69938177,-0.25650084,0.44456664,0.9730888,-0.38067976,-0.20776,-0.4901542,0.65263367,0.15264961,0.28978726,1.0760404,0.985356,-0.18591347,1.4820762,-0.5612234,-0.18539436,0.040458065,-0.06148353,-0.54066044,0.16859727,0.9160127,0.4443738,0.15513156,0.26086074,-0.23979916,-0.10413779,0.23503867,-0.7062806,0.40655932,-0.3828966,0.53477544,-0.76662725,0.019249544,0.163788,0.34116694,0.14305028,0.10919537,-0.07697471,0.053621925,-0.14774406,0.49686512,1.0013852,0.330999,0.20489115,-0.07479863,-0.70555043,-0.59187436,0.023208153,-0.784217,-0.2086444,-0.08256975,0.38271224,0.73024285,0.42666999,0.23859897,0.49071372,0.6518053,-0.11779735,0.31844163,0.82380724,0.41055378,-0.31385192,0.36048335,0.21993056,-0.4547515,0.21402921,0.46906197,-0.010404289,-0.034571033,-0.11041315,0.19220428,1.6250243,0.434003,-0.14742555,0.5164056,0.3704263,0.20215431,-0.29232323,-1.0828396,0.09933846,-0.2926279,0.3887126,-0.5572146,-0.6148859,0.40866736,0.4752134,0.08526811,-0.040201228,-0.073706925,0.075207,0.19704354,-0.24850154,0.10333117,0.11240796,-0.10289113,-0.22215346,0.018319115,0.24484445,0.5875986,-0.001396127,-0.6715586,-0.49839938,0.84074277,-0.0767863,-0.26675248,0.28035533,-0.67623335,-0.263601,-0.6688575,0.18970585,-0.5487923,0.8651192,-0.15903415,-0.8387729,-0.11012623,-0.5398837,1.6945384,0.03899939,0.49038184,0.54203624,-0.039890908,-0.297378,-0.1514954,0.43277514,-0.29324028,-0.26331797,-0.26841545,-0.69370955,0.54141575,-0.219119,-0.121275686,0.25459477,-0.38255924,-0.14859927,0.4139896,-0.1518263,0.7788728,-0.5210492,-0.052708372,0.52959955,0.23969384,-2.5637732,-0.3646096,0.088783875,0.16927765,-0.20440555,0.014467901,0.9040701,-0.24180907,0.04352094,-0.59232676,0.89461696,0.22022966,-0.080807015,-0.71604913,0.12687609,0.5447235,-0.41935638,-0.37054712,-0.3010311,-0.6222862,-0.36154068,-0.19361559,1.4531188,0.3193091,0.17359017,-0.37251654,0.5885344,-0.5304642,-0.90737766,1.0579817,0.37693805,-0.110948555,0.43083888,-0.47653425,-0.65940446,0.17339955,1.35262,0.079714835,-0.07835302,-0.24392623,1.2289393,-0.5184187,-0.1307936,-0.007878594,0.6055258,0.055373766,0.34789962,0.320513,-0.018046744,0.09805898,0.0036491752,-0.28539872,-0.42375314,-0.0008817464,-0.29914862,0.5400663,-0.17257236,0.62332153,0.612576,0.18791531,0.022848241,-0.32972604,-0.13474633,-0.8564286,0.3612426],"result":{"type":"object","properties":{"articles":{"items":{"properties":{"authors":{"items":{"type":"string"},"type":"array"},"publish_date":{"type":"string"},"source":{"type":"string"},"summary":{"type":"string"},"text":{"type":"string"},"title":{"type":"string"},"top_image":{"type":"string"},"url":{"type":"string"}},"type":"object"},"type":"array"},"failed_sources":{"items":{"type":"string"},"type":"array"},"processing_time":{"type":"number"},"total_articles_found":{"description":"Total number of articles found across all sources","type":"integer"},"total_sources_processed":{"description":"Number of news sources that were processed","type":"integer"}},"required":["total_sources_processed","total_articles_found","failed_sources","articles","processing_time"]},"sql_tables":null,"sql_queries":null,"file_inbox":null,"oauth":null,"assets":null,"runner":"any","operating_system":["linux","macos","windows"],"tool_set":""},false]}